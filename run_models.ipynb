{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "from sleep_classif.CNNmultitaper import ConvNetMultitaper\n",
    "from sleep_classif.LSTMConv import LSTM_Conv\n",
    "from sleep_classif.CNNadvanced import CNN_Advanced\n",
    "from sleep_classif.CNNmodel import SimpleCNN\n",
    "\n",
    "# import loaders and other functions\n",
    "from sleep_classif.preprocessing import compute_tapers\n",
    "from sleep_classif.dataloaders import MultiTaperSet, RawDataSet, FFT_Raw_DataSet\n",
    "from sleep_classif.trainer import Trainer\n",
    "\n",
    "# import from other librairies \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_fold indices generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_indices(set_size, n_folds = 5):\n",
    "    '''\n",
    "    return list of folds indices (train indices and  test indices)\n",
    "    '''\n",
    "    s = list(range(0, set_size))\n",
    "    random.shuffle(s)\n",
    "    s = [s[i::n_folds] for i in range(n_folds)]\n",
    "    folds = []\n",
    "    for i in range(n_folds):\n",
    "        test_set = np.array(s[i])\n",
    "        train_set = np.array([s[j] for j in range(n_folds) if i!=j]).ravel()\n",
    "        folds.append((train_set, test_set))\n",
    "    return(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([7, 2, 8, 6, 4, 3, 5, 0]), array([1, 9])),\n",
       " (array([1, 9, 8, 6, 4, 3, 5, 0]), array([7, 2])),\n",
       " (array([1, 9, 7, 2, 4, 3, 5, 0]), array([8, 6])),\n",
       " (array([1, 9, 7, 2, 8, 6, 5, 0]), array([4, 3])),\n",
       " (array([1, 9, 7, 2, 8, 6, 4, 3]), array([5, 0]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold_indices(10, n_folds = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train basic CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\Deep Learning\\deep_learning_project\\venv\\lib\\site-packages\\sleep_classif-0.0.0-py3.8.egg\\sleep_classif\\dataloaders.py:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n"
     ]
    }
   ],
   "source": [
    "data_path_train = './data/raw_data/X_train.h5'\n",
    "target_path = './data/raw_data/y_train.csv'\n",
    "\n",
    "raw_train_set = RawDataSet(device=device,\n",
    "                                 data_path = data_path_train,\n",
    "                                 target_path = target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\Deep Learning\\deep_learning_project\\venv\\lib\\site-packages\\sleep_classif-0.0.0-py3.8.egg\\sleep_classif\\trainer.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:17<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average train loss:  1.4166004012344748\n",
      "validation loss:  1.575417628771142\n",
      "validation accuracy:  0.2656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:13<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average train loss:  1.3620418978344864\n",
      "validation loss:  1.6330611781228948\n",
      "validation accuracy:  0.254\n",
      "fold number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:14<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average train loss:  1.3567340487887145\n",
      "validation loss:  1.4563542634625979\n",
      "validation accuracy:  0.4342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:13<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average train loss:  1.3420212094191533\n",
      "validation loss:  1.4792934942849074\n",
      "validation accuracy:  0.4132\n",
      "fold number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:14<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average train loss:  1.3375354785068778\n",
      "validation loss:  1.5071545673322073\n",
      "validation accuracy:  0.3872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:14<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average train loss:  1.3263680266726547\n",
      "validation loss:  1.5094483230687394\n",
      "validation accuracy:  0.3832\n",
      "K_fold average accuracy: 0.35013333333333335\n"
     ]
    }
   ],
   "source": [
    "### train with K_fold \n",
    "\n",
    "acc_fold_list = []\n",
    "n_folds = 5\n",
    "n_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "for n_fold, (train_indices, val_indices) in enumerate(k_fold_indices(len(raw_train_set), n_folds = n_folds)):\n",
    "        print(f\"fold number: {n_fold+1}\")\n",
    "        #instanciate a new model \n",
    "        simple_cnn = SimpleCNN().to(device)\n",
    "        optimizer = torch.optim.Adam(simple_cnn.parameters())\n",
    "\n",
    "        # creata data sets\n",
    "        train_set = torch.utils.data.dataset.Subset(raw_train_set,train_indices)\n",
    "        val_set = torch.utils.data.dataset.Subset(raw_train_set,val_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # instanciate trainer\n",
    "        trainer = Trainer(simple_cnn,\n",
    "                        nn.CrossEntropyLoss(),\n",
    "                        optimizer,\n",
    "                        train_loader,\n",
    "                        device,\n",
    "                        valid_data_loader = val_loader,\n",
    "                        class_weights=torch.Tensor([8.081897,22.222222, 2.756846, 3.765060, 4.927727]))\n",
    "\n",
    "        # train model\n",
    "        loss_list = []\n",
    "        accuracy_list = []\n",
    "        for epoch in range(0,n_epochs):\n",
    "                loss, accuracy = trainer.train_epoch()\n",
    "                if epoch == n_epochs-1:\n",
    "                        acc_fold_list.append(accuracy)\n",
    "\n",
    "# return mean accuracy\n",
    "accuracy = np.mean(acc_fold_list)\n",
    "print(f\"K_fold average accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN + Multitaper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MultiTapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_tapers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_eeg_path_train = './data/pre_processed_data/Multitaper_eeg_train.npy'\n",
    "features_position_path_train = './data/pre_processed_data/Multitaper_position_train.npy'\n",
    "\n",
    "target_path = './data/raw_data/y_train.csv'\n",
    "\n",
    "taper_train_set = MultiTaperSet(device=device,\n",
    "                                features_eeg_path = features_eeg_path_train,\n",
    "                                features_position_path = features_position_path_train,\n",
    "                                target_path = target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train with K_fold \n",
    "\n",
    "acc_fold_list = []\n",
    "n_folds = 5\n",
    "n_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "for n_fold, (train_indices, val_indices) in enumerate(k_fold_indices(len(taper_train_set), n_folds = n_folds)):\n",
    "        print(f\"fold number: {n_fold+1}\")\n",
    "        #instanciate a new model \n",
    "        CNN_taper_model = ConvNetMultitaper().to(device)\n",
    "        optimizer = torch.optim.Adam(CNN_taper_model.parameters())\n",
    "\n",
    "        # creata data sets\n",
    "        train_set = torch.utils.data.dataset.Subset(taper_train_set,train_indices)\n",
    "        val_set = torch.utils.data.dataset.Subset(taper_train_set,val_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # instanciate trainer\n",
    "        trainer = Trainer(CNN_taper_model,\n",
    "                        nn.CrossEntropyLoss(),\n",
    "                        optimizer,\n",
    "                        train_loader,\n",
    "                        device,\n",
    "                        valid_data_loader = val_loader,\n",
    "                        class_weights=torch.Tensor([8.081897,22.222222, 2.756846, 3.765060, 4.927727]))\n",
    "\n",
    "        # train model\n",
    "        loss_list = []\n",
    "        accuracy_list = []\n",
    "        for epoch in range(0,n_epochs):\n",
    "                loss, accuracy = trainer.train_epoch()\n",
    "                if epoch == n_epochs-1:\n",
    "                        acc_fold_list.append(accuracy)\n",
    "\n",
    "# return mean accuracy\n",
    "accuracy = np.mean(acc_fold_list)\n",
    "print(f\"K_fold average accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an advanced CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "\n",
    "data_path_train = './data/raw_data/X_train.h5'\n",
    "target_path = './data/raw_data/y_train.csv'\n",
    "\n",
    "\n",
    "\n",
    "raw_train_set = FFT_Raw_DataSet(device=device,\n",
    "                                 data_path = data_path_train,\n",
    "                                 target_path = target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train with K_fold \n",
    "\n",
    "acc_fold_list = []\n",
    "n_folds = 5\n",
    "n_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "num_classes = 5\n",
    "raw_feat, fft_feat, raw_pos_feat, fft_pos_feat = 5,5,3,3\n",
    "\n",
    "for n_fold, (train_indices, val_indices) in enumerate(k_fold_indices(len(raw_train_set), n_folds = n_folds)):\n",
    "        print(f\"fold number: {n_fold+1}\")\n",
    "        #instanciate a new model \n",
    "        CNN_Advanced_model = CNN_Advanced(raw_feat, fft_feat, raw_pos_feat, fft_pos_feat, num_classes, 0.5).to(device)\n",
    "        optimizer = torch.optim.Adam(CNN_Advanced_model.parameters())\n",
    "\n",
    "        # creata data sets\n",
    "        train_set = torch.utils.data.dataset.Subset(raw_train_set,train_indices)\n",
    "        val_set = torch.utils.data.dataset.Subset(raw_train_set,val_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # instanciate trainer\n",
    "        trainer = Trainer(CNN_Advanced_model,\n",
    "                        nn.CrossEntropyLoss(),\n",
    "                        optimizer,\n",
    "                        train_loader,\n",
    "                        device,\n",
    "                        valid_data_loader = val_loader,\n",
    "                        class_weights=torch.Tensor([8.081897,22.222222, 2.756846, 3.765060, 4.927727]))\n",
    "\n",
    "        # train model\n",
    "        loss_list = []\n",
    "        accuracy_list = []\n",
    "        for epoch in range(0,n_epochs):\n",
    "                loss, accuracy = trainer.train_epoch()\n",
    "                if epoch == n_epochs-1:\n",
    "                        acc_fold_list.append(accuracy)\n",
    "\n",
    "# return mean accuracy\n",
    "accuracy = np.mean(acc_fold_list)\n",
    "print(f\"K_fold average accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = './data/raw_data/X_train.h5'\n",
    "\n",
    "target_path = './data/raw_data/y_train.csv'\n",
    "\n",
    "\n",
    "\n",
    "raw_train_set = RawDataSet(device=device,\n",
    "                                 data_path = data_path_train,\n",
    "                                 target_path = target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train with K_fold \n",
    "\n",
    "acc_fold_list = []\n",
    "n_folds = 5\n",
    "n_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "raw_feat = raw_train_set.feature_shape()\n",
    "num_classes = 5\n",
    "\n",
    "for n_fold, (train_indices, val_indices) in enumerate(k_fold_indices(len(raw_train_set), n_folds = n_folds)):\n",
    "        print(f\"fold number: {n_fold+1}\")\n",
    "        #instanciate a new model \n",
    "        LSTM_Conv_model = LSTM_Conv(raw_feat, num_classes).to(device)\n",
    "        optimizer = torch.optim.Adam(LSTM_Conv_model.parameters())\n",
    "\n",
    "        # creata data sets\n",
    "        train_set = torch.utils.data.dataset.Subset(raw_train_set,train_indices)\n",
    "        val_set = torch.utils.data.dataset.Subset(raw_train_set,val_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # instanciate trainer\n",
    "        trainer = Trainer(LSTM_Conv_model,\n",
    "                        nn.CrossEntropyLoss(),\n",
    "                        optimizer,\n",
    "                        train_loader,\n",
    "                        device,\n",
    "                        valid_data_loader = val_loader,\n",
    "                        class_weights=torch.Tensor([8.081897,22.222222, 2.756846, 3.765060, 4.927727]))\n",
    "\n",
    "        # train model\n",
    "        loss_list = []\n",
    "        accuracy_list = []\n",
    "        for epoch in range(0,n_epochs):\n",
    "                loss, accuracy = trainer.train_epoch()\n",
    "                if epoch == n_epochs-1:\n",
    "                        acc_fold_list.append(accuracy)\n",
    "\n",
    "# return mean accuracy\n",
    "accuracy = np.mean(acc_fold_list)\n",
    "print(f\"K_fold average accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-python38-py",
   "name": "common-cu110.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
  },
  "interpreter": {
   "hash": "90400e2509a7f97e1574e9e217fc898b753a8c591c8f81400b72bc0569fd4fc5"
  },
  "kernelspec": {
   "display_name": "Python [conda env:python38]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
